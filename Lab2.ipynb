{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d41bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import math\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib as plt\n",
    "from matplotlib import pyplot as plt\n",
    "import tabulate \n",
    "from tabulate import tabulate as tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a104bb7",
   "metadata": {},
   "source": [
    "2.4.1 - The expected roundtrip time of light in a fiberoptic cable 2000km would be (delta t) = (2000km)/(2.054x10^8m/s) = .01 seconds. The uncertainty in this measurment comes from the uncertainty in the measurment of the speed of light from the internet that I used, although I could not find an accompanying uncertianty. It would be difficult to predict the latency of this measurment assuming it was a \"ping\" type of measurment, seeing as nothing can exceed the speed of light. \n",
    "\n",
    "\n",
    "To measure using two cable connected computers would require a lot of resolution as the time it takes to receive the message and process could possibly exceed the time it takes to actually get there. Assuming the light travels at 2.054x10^8 m/s, and it is traveling 1 m we would need 10^-8 s resolution, which is tiny, for 20 meters by the same logic we would need 10^-7 s of resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6340003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_ntu = pd.read_csv('/Users/henryaxon/Downloads/391Files/Lab2/NTUData2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd46266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_cern = pd.read_csv('/Users/henryaxon/Downloads/391Files/Lab2/CernData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a039e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_UQ = pd.read_csv('/Users/henryaxon/Downloads/391Files/Lab2/UQData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a18e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_Berkeley = pd.read_csv('/Users/henryaxon/Downloads/391Files/Lab2/BerkeleyPingData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dc75693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_Columbia = pd.read_csv('/Users/henryaxon/Downloads/391Files/Lab2/ColumbiaPingData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ff02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_Columbia.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d1b667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ColumbiaArray = Data_Columbia.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b10e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_csv(file_location: str):\n",
    "    list = []\n",
    "    file = open(file_location,\"r\")\n",
    "    for line in file:\n",
    "        line = line.split ()\n",
    "        if len(line) == 9:\n",
    "            time = line[-2]\n",
    "            time.strip()\n",
    "            time_float = time[5:]\n",
    "            ftime = float(time_float)\n",
    "            list.append(ftime)\n",
    "    data = pd.DataFrame(list)\n",
    "    data.to_csv(f'{file_location [:-4]}.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ce178",
   "metadata": {},
   "source": [
    "Converting .txt to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d7957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_csv('NTUData2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecf0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_csv('CernData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e54287cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_csv('UQData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f750135",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_csv('BerkeleyPingData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca718cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_csv('ColumbiaPingData.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c4eaf",
   "metadata": {},
   "source": [
    "How was data analyzed 2.4.3 Ping Time Analysis \n",
    "\n",
    "To get the data I simply pinged 5 locations 100 times and copied and pasted the data into a document which was then turned into a .txt and read into the notebook as a .csv. The time data was seperated, and then I plotted histograms just to roughly see what exactly the distribution of the ping times was. Then I simply analyzed the histograms to find the mean times, standard deviation, and the uncertianties on the time measurements. \n",
    "\n",
    "The next step was to indtroduce distance measurements taken with a combination of undersea cable maps, google maps and traceroute. This allowed me to get a sense for the distance as well as estimate the uncertainty in the distance measurement. This data was then used with the time data to find the speed of light extracted from a polyfit. Error propagation was done using the standard partial derivatives added in quadrature formula, and then all of this was used to find a weighted average of the data as well as a weighted uncertainty on this average value. Finally a correction was made to turnt he value into the speed of light in a vaccum.\n",
    "\n",
    "\n",
    "2.5\n",
    "Note: The time being measured is the two way time to each location and so to find the speed of light the distance was doubled to find the actual distance travelled vs the total time. My calcualted values support this being a reasonable approach.\n",
    "\n",
    "2.5.2 \n",
    "\n",
    "\n",
    "There are many reasons that data could have outliers or be wierdly longer or shorter than expected. One possibility would be the network traffic in these locations. For example Columbia, and NTU are in two of the most densely populated and technologicaly advanced cities on earth (NY and 台北）， additional traffic could increase the latency times. Beyond that possibility, perhaps the most important thing to consider is that the route taken by pings is not direct and depends on the network of fiber optic cabels crisscrossing the globe. In particular when I pinged Oregon State as a test run, the traceroute data showed that the signal was sent from Eugene, to Portland, to Boise, To Salt Lake etc. This would add hundreds of miles to the distance and thus would greatly extend the time that the Ping would return, in some cases making a relatively short trip display times longer than much longer ping distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9fca67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataNTU = pd.read_csv('NTUData2.csv').values\n",
    "DataCern = pd.read_csv('CernData.csv').values\n",
    "DataUQ = pd.read_csv('UQData.csv').values\n",
    "DataBerkeley = pd.read_csv('BerkeleyPingData.csv').values\n",
    "DataColumbia = pd.read_csv('ColumbiaPingData.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32afcf93",
   "metadata": {},
   "source": [
    "Extract the time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeNTU = DataNTU[::,1]\n",
    "TimeCern = DataCern[::,1]\n",
    "TimeUQ = DataUQ[::,1]\n",
    "TimeBerkeley = DataBerkeley[::,1]\n",
    "TimeColumbia = DataColumbia[::,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554eb0dd",
   "metadata": {},
   "source": [
    "Finding mean times at each location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MeanNTUT = np.mean(TimeNTU)\n",
    "MeanCernT = np.mean(TimeCern)\n",
    "MeanUQT = np.mean(TimeUQ)\n",
    "MeanBerkeleyT = np.mean(TimeBerkeley)\n",
    "MeanColumbiaT = np.mean(TimeColumbia)\n",
    "MeanTs = [MeanNTUT,MeanCernT,MeanUQT,MeanBerkeleyT,MeanColumbiaT]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec49418",
   "metadata": {},
   "source": [
    "Finding standard deviation of the time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SigmaNTUT = np.std(TimeNTU)\n",
    "SigmaCernT = np.std(TimeCern)\n",
    "SigmaUQT = np.std(TimeUQ)\n",
    "SigmaBerkeleyT = np.std(TimeBerkeley)\n",
    "SigmaColumbiaT = np.std(TimeColumbia)\n",
    "Sigmas = [SigmaNTUT,SigmaCernT,SigmaUQT,SigmaBerkeleyT,SigmaColumbiaT]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00cc210",
   "metadata": {},
   "source": [
    "Finding error from the Std deviation, Std/sqrt(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorNTUT = (SigmaNTUT)/((100)**.5)\n",
    "ErrorCernT = (SigmaCernT)/((100)**.5)\n",
    "ErrorUQT = (SigmaUQT)/((100)**.5)\n",
    "ErrorBerkeleyT = (SigmaBerkeleyT)/((100)**.5)\n",
    "ErrorColumbiaT = (SigmaColumbiaT)/((100)**.5)\n",
    "Errors1 = [ErrorNTUT,ErrorCernT,ErrorUQT,ErrorBerkeleyT,ErrorColumbiaT]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d34aa",
   "metadata": {},
   "source": [
    "Plotting Histograms to verify the plausibility of the data being correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387caa93",
   "metadata": {},
   "source": [
    "NTU Data Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(TimeNTU) #This looks reasonable as all ping times(other than outliers) all line up in one bin\n",
    "plt.title(\"NTU Ping\")\n",
    "plt.xlabel(\"Ping Times (ms)\")\n",
    "plt.ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068bce28",
   "metadata": {},
   "source": [
    "Cern Data Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(TimeCern, bins = 5)\n",
    "plt.title(\"Cern Ping\")\n",
    "plt.xlabel(\"Ping Times (ms)\")\n",
    "plt.ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc33c1",
   "metadata": {},
   "source": [
    "University of Queensland Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(TimeUQ)\n",
    "plt.title(\"UQ Ping\")\n",
    "plt.xlabel(\"Ping Times (ms)\")\n",
    "plt.ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea4744",
   "metadata": {},
   "source": [
    "Berkeley Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(TimeBerkeley)\n",
    "plt.title(\"Berkeley Ping\")\n",
    "plt.xlabel(\"Ping Times (ms)\")\n",
    "plt.ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a466c1f",
   "metadata": {},
   "source": [
    "Columbia Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(TimeColumbia)\n",
    "plt.title(\"Columbia Ping\")\n",
    "plt.xlabel(\"Ping Times (ms)\")\n",
    "plt.ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d203e6",
   "metadata": {},
   "source": [
    "I would say that all of the data above appears reasonable, Cern has a fairly signifigant outlier which seemed to be common in the other people I talked to's data, however I may include it, or at least include it initially. Interestingly, the data outside of this one data point was perhaps the most consistent of any location, but the single error is much further outside the mean than any other point in the other data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebc3af",
   "metadata": {},
   "source": [
    "Printing best estimates for Mean times for pings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best estimate for ping time to NTU is:\",MeanNTUT, \"+/-\", ErrorNTUT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best estimate for ping time to Cern is:\",MeanCernT, \"+/-\", ErrorCernT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f384ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best estimate for ping time to UQ is:\",MeanUQT, \"+/-\", ErrorUQT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best estimate for ping time to Berkeley is:\",MeanBerkeleyT, \"+/-\", ErrorBerkeleyT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b1c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best estimate for ping time to Columbia is:\",MeanColumbiaT, \"+/-\", ErrorColumbiaT )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0d31b",
   "metadata": {},
   "source": [
    "Creating a table of ping time \"best estimates\", I generated a new table below with distances which may not have been the best strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table = [['Location', 'Avg Ping(ms)', 'STD Devation(ms)', 'Error/Uncertainty(ms)'],\n",
    "        [\"NTU\", round(MeanNTUT,1), round(SigmaNTUT,1), round(ErrorNTUT,2)],\n",
    "        [\"Cern\", round(MeanCernT,1), round(SigmaCernT,1), round(ErrorCernT,2)],\n",
    "        [\"UQ\", round(MeanUQT,1), round(SigmaUQT,1), round(ErrorUQT,2)],\n",
    "        [\"Berkeley\", round(MeanBerkeleyT,1), round(SigmaBerkeleyT,1), round(ErrorBerkeleyT,2)],\n",
    "        [\"Columbia\", round(MeanColumbiaT,1), round(SigmaColumbiaT,1), round(ErrorColumbiaT,2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f17fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tab(Table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858e38c",
   "metadata": {},
   "source": [
    "2.6 Distance Analysis\n",
    "\n",
    "Distances: Total distance travelled is 2 times the distance travelled.\n",
    "NTU = 2(11806km El segundo + 1428km eugene) \n",
    "Cern = 2(6400km to france +695km to cern)\n",
    "UQ = 2(13000km to NSW au + 900km Bris + 1428km to el seg)\n",
    "Berkeley = 2(907km)\n",
    "Columbia = 2(4600km)\n",
    "\n",
    "\n",
    "as far as uncertainty, because we used a combination of google maps, traceeroute function and submarine cables underground. The uncertainty is very very high in all of these measurements due to most of the locations being in very large cities, as well as being a long ways away with no way of knowing the exact route on land, and thus the distances not covered by the submarine cable map were estimated using walking directions on google maps but traceroute doesnt tell us what the exact route is. Based on this lack of accuracy and precision, I would say that we have probably a percent uncertainty of 25%, thus the fractional error on each measurement is estimated using the 25% estimate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3583dec",
   "metadata": {},
   "source": [
    "Total Distance travelled to each point is calculated below, using the traceroute function to estimate the distance travelled and underground cable maps for the international travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeacdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DistanceNTU = 2*(11806+1428)\n",
    "DistanceCern = 2*(6400+695)\n",
    "DistanceUQ = 2*(13000+900+1428)\n",
    "DistanceBerkeley = 2*(907)\n",
    "DistanceColumbia = 2*(4600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93e945",
   "metadata": {},
   "source": [
    "I then used the 25% estimate to find the fractional uncertainty on distance for each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "FractionalNTUD = round(DistanceNTU * .25, 3)\n",
    "FractionalCernD = round(DistanceCern * .25,3)\n",
    "FractionalUQD = round(DistanceUQ * .25,3)\n",
    "FractionalBerkeleyD = round(DistanceBerkeley * .25,3)\n",
    "FractionalColumbiaD = round(DistanceColumbia * .25, 3)\n",
    "\n",
    "FractionalsD = [FractionalNTUD,FractionalCernD,FractionalUQD,FractionalBerkeleyD,FractionalColumbiaD]\n",
    "FractionalsT = [ErrorNTUT, ErrorCernT, ErrorUQT, ErrorBerkeleyT, ErrorColumbiaT]\n",
    "Distances = [DistanceNTU,DistanceCern,DistanceUQ,DistanceBerkeley, DistanceColumbia]\n",
    "Times = [MeanNTUT, MeanCernT, MeanUQT, MeanBerkeleyT,MeanColumbiaT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table2 = [['Location', 'Avg Ping(ms)', 'STD Devation(ms)', 'Ping Error/Uncertainty(ms)', 'Distance(km)', 'Distance Uncertainty(km)'],\n",
    "        [\"NTU\", round(MeanNTUT,3), round(SigmaNTUT,3), round(ErrorNTUT,3), round(DistanceNTU,3), round(FractionalNTUD,3)],\n",
    "        [\"Cern\", round(MeanCernT,3), round(SigmaCernT,3), round(ErrorCernT,3), round(DistanceCern,3),round(FractionalCernD,3) ],\n",
    "        [\"UQ\", round(MeanUQT,3), round(SigmaUQT,3), round(ErrorUQT,3), round(DistanceUQ,3), round(FractionalUQD,3)],\n",
    "        [\"Berkeley\", round(MeanBerkeleyT,3), round(SigmaBerkeleyT,3), round(ErrorBerkeleyT,3), round(DistanceBerkeley,3),round(FractionalBerkeleyD,3)],\n",
    "        [\"Columbia\", round(MeanColumbiaT,3), round(SigmaColumbiaT,3), round(ErrorColumbiaT,3), round(DistanceColumbia,3),round(FractionalColumbiaD,3) ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ee6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tab(Table2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d582fb",
   "metadata": {},
   "source": [
    "2.7-Speed of Light\n",
    "\n",
    "\n",
    "\n",
    "Plotting the distances vs the times and fitting the line to get a speed of light estimate. As we can see the fractional errors on the measurements seem to align well with the fit with the exception of 1 point which is Cern. The reason for this is that it seems that perhaps due to more domestic redirections the distance may have been underestimated which would lead us to calculate a Cern speed of light much lower than the other estimates which lie within the range of the uncertainty.\n",
    "\n",
    "Zero ping is not possible for many reasons, and is affected by everything from network traffic to just the simople fact that the message has to send be processed and information returned all of which affects the latency and causes the time to be longer than if we were simply sending a light beam that is instantly processed. Latency could be increased by amplifiers, particularly over the long distances that we are working with.\n",
    "\n",
    "\n",
    "The measurment of distance is much more uncertain as again we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d69ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(Times, Distances, color = 'purple')\n",
    "a, b = np.polyfit(Times,Distances , 1)\n",
    "plt.plot(Times, a*np.array(Times)+b, label= (\"\"),color = 'g')\n",
    "plt.errorbar(Times, Distances, yerr=FractionalsD, xerr=FractionalsT, fmt='ro', ecolor=None, elinewidth=None)\n",
    "plt.xlabel('Times(ms)')\n",
    "plt.ylabel('Distance(km)')\n",
    "plt.title('Distance Travelled vs Ping Time')\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41046c4",
   "metadata": {},
   "source": [
    "This plot seems to suggest that the 25% estimate based on the large uncertianty estimated using traceroute was about right. The data also suggests that the time measurements, are generally far more precise(also as expected). The exception being as discussed earlier (CERN) which has much larger error due to the one outlier impacting the calcualtion of time error bars. The issue is certianly the outlier point but even after taking multiple data sets the outlier seemed to stick around. \n",
    "\n",
    "The other data points seem to fit quite well and if I were to remove CERN might fit even better. \n",
    "\n",
    "The data as a whole certainly seems to support a linear fit being correct, however, again CERN seems to be an exception disrupting what is otherwise on the surface very linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700ff52",
   "metadata": {},
   "source": [
    "Finding the Speed of light through the fiber optic cables at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different V values for each location\n",
    "VNTU = DistanceNTU/MeanNTUT\n",
    "VCern = DistanceCern/MeanCernT\n",
    "VUQ = DistanceUQ/MeanUQT\n",
    "VBerkeley = DistanceBerkeley/MeanBerkeleyT\n",
    "VColumbia = DistanceColumbia/MeanColumbiaT\n",
    "\n",
    "Vn = [VNTU,VCern,VUQ,VBerkeley,VColumbia]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aeb846",
   "metadata": {},
   "source": [
    "Error Propagation: \n",
    "V = D/t , so partial derivatives for with respect to D and T, (dV/dD) = 1/t, (dV/dt) = -D/t^2, now we just have to plug into the adding in quadrature equation with the uncertainties I found earlier. This will give us absolute uncertianty in our calcluation of Vn for each location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance derivatives\n",
    "DDNTU = 1/MeanNTUT\n",
    "DDCern = 1/MeanCernT\n",
    "DDUQ = 1/MeanUQT\n",
    "DDBerkeley = 1/MeanBerkeleyT\n",
    "DDColumbia = 1/MeanColumbiaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Derivatives\n",
    "DtNTU = (-DistanceNTU)/(MeanNTUT**2)\n",
    "DtCern = (-DistanceNTU)/(MeanNTUT**2)\n",
    "DtUQ = (-DistanceNTU)/(MeanNTUT**2)\n",
    "DtBerkeley = (-DistanceNTU)/(MeanNTUT**2)\n",
    "DtColumbia = (-DistanceNTU)/(MeanNTUT**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add in Quadrature\n",
    "ErrorNTUV = math.sqrt(((DDNTU*FractionalNTUD)**2)+ ((DtNTU*ErrorNTUT)**2))\n",
    "ErrorCernV = math.sqrt(((DDCern*FractionalCernD)**2)+ ((DtCern*ErrorCernT)**2))\n",
    "ErrorUQV = math.sqrt(((DDUQ*FractionalUQD)**2)+ ((DtUQ*ErrorUQT)**2))\n",
    "ErrorBerkeleyV = math.sqrt(((DDBerkeley*FractionalBerkeleyD)**2)+ ((DtBerkeley*ErrorBerkeleyT)**2))\n",
    "ErrorColumbiaV = math.sqrt(((DDColumbia*FractionalColumbiaD)**2)+ ((DtColumbia*ErrorColumbiaT)**2))\n",
    "ErrorV = [ErrorNTUV,ErrorCernV,ErrorUQV,ErrorBerkeleyV,ErrorColumbiaV]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46756445",
   "metadata": {},
   "source": [
    "2.7.1 There are many reasons a 0 ping time is not possible, even at low distances there will be a processing time that the computers will need. And so while in theory we might be able to get very close to 0, there is always latency, and the light does still need to travel the distance which requires some time, although perhaps that time difference would not be resolvable on our computers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Table of measured speed of light values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c26ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TableV = [['Location', 'Calculated V(km/ms)', 'Error on V(km/ms)'],\n",
    "         ['NTU',round(VNTU,3),round(ErrorNTUV,3)],\n",
    "         ['Cern', round(VCern,3), round(ErrorCernV,3)],\n",
    "         ['UQ',round(VUQ,3), round(ErrorUQV,3)],\n",
    "         ['Berkeley', round(VBerkeley,3), round(ErrorBerkeleyV,3)],\n",
    "         ['Columbia', round(VColumbia,3), round(ErrorColumbiaV,3) ],\n",
    "         ['Slope Based V', round(a,3), ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tab(TableV))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e0561",
   "metadata": {},
   "source": [
    "Vn vs distance plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531acde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Distances,Vn)\n",
    "a, b = np.polyfit(Distances,Vn,1)\n",
    "plt.plot(Distances, a*np.array(Distances)+b)\n",
    "plt.xlabel(\"Distance Travelled(km)\")\n",
    "plt.ylabel(\"Speed of light through cables(km/ms)\")\n",
    "plt.title('Speed of Light Through Cables vs Distances')\n",
    "print(a, b)\n",
    "\n",
    "plt.errorbar(Distances, Vn, yerr=ErrorV ,fmt='ro', ecolor=None, elinewidth=None )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea488079",
   "metadata": {},
   "source": [
    "The Vn values appear to be roughly consistent within the errors, even if just barely. I think its quite possible that my 25% measurement probably greatly overestimated some of the error, btu this new total error seems to fit the data quite well other than what is a clear outlier (CERN). I would say that the plot shows that my errors are reasonable given the spread and inconsistencies within my data itself, and on this plot after propogation they seem to be an overestimate. However they seem to show that the trend itself is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c398b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "239fad55",
   "metadata": {},
   "source": [
    "2.7.4. Weighted Average\n",
    "\n",
    "\n",
    "xw = (sum[vn*weight])/(sum(w))\n",
    "\n",
    "\n",
    "w = 1/sigma^2\n",
    "\n",
    "\n",
    "\n",
    "sigmaWA = 1/sqrt(sum(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f51d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(measurements, uncertainties):\n",
    "    if len(measurements) != len(uncertainties):\n",
    "        raise ValueError(\"Input lists must have the same length\")\n",
    "    \n",
    "    weighted_sum = 0\n",
    "    weight_sum = 0\n",
    "    \n",
    "    for measurement, uncertainty in zip(measurements, uncertainties):\n",
    "        weight = 1 / (uncertainty ** 2)\n",
    "        weighted_sum += measurement * weight\n",
    "        weight_sum += weight\n",
    "\n",
    "    if weight_sum == 0:\n",
    "        raise ValueError(\"Total weight is zero, which would result in division by zero.\")\n",
    "    \n",
    "    weighted_avg = round(weighted_sum / weight_sum, 1)\n",
    "    weighted_avg_unc = round(1/(weight_sum**.5), 1)\n",
    "    return print(\"weighted average of Vn is \",weighted_avg, \"km/ms, and uncertainty is\", weighted_avg_unc, \"km/ms\")\n",
    "\n",
    "# Example usage:\n",
    "#measurements = [10, 15, 20, 25]\n",
    "#uncertainties = [1, 2, 1.5, 2.5]\n",
    "\n",
    "#result = weighted_average(measurements, uncertainties)\n",
    "#print(\"Weighted Average:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdee8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average(Vn, ErrorV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400df500",
   "metadata": {},
   "source": [
    "2.8\n",
    "\n",
    "Correction of data:\n",
    "\n",
    "\n",
    "Correction of data involves going from the speed of light in a fiber to the speed of light in a vaccum which requires a correction of the weighted average by multiplying by the index of refraction of glass/fiberoptic cables. Which is n=1.5. And this should give us the experimentally measured speed of light in a vaccum. This was done below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4656d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1.5\n",
    "WeightedVn = 114.9\n",
    "Vvaccum = round(WeightedVn*n,1)\n",
    "ErrorC = round(13.5*1.5)\n",
    "print(Vvaccum, \"km/ms +/- 13.5 km/ms\")\n",
    "print(ErrorC)\n",
    "print(\" Measured Speed of light in a vaccum is 1.72e+8 +/- 0.2e+8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a86a6",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "    \n",
    "    \n",
    "Comparing the measured c to the expected value of 3x10^8, we can see that there is a noticeable discrepancy between the measured value and the expected. Fortunately we are in the same order of magnitude of 10^8. However technichally the actual speed of light would lie outside of the uncertianty calculated for the measurements of the speed of light. I think that it is possible that I was underestimating the uncertainties that were involved in the time measurements. This is because the majority of my data was collected from Institutions outside of the US, and so there would be errors surrounding traffic in the US vs in Europe, or the scale of the institution being pinged. All of these schools have thousands of students and Cern has a very high load as well. This would introduce more latency to the time measurement which after reflection probably very signifiganty impacted my measurements of Vn. \n",
    "\n",
    " I repeated this experiment I might attempt to find a way to relate latency times to the distance to compensate for things like amplifiers, and traffic elsewhere. I also could try to spread my data collection out in such a way that I could measure at low traffic times for every location as opposed to just collecting the majority of my data in one short time period that doesnt take into account the time zone difference. This could explains some of why CERN data was so strange as it has a much differnet time zone than all of the other data whihc is either in the USA, or in Asia, so maybe I caught CERN at an extra busy network time period in the day, whereas people in Taiwan may have been asleep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a450e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
